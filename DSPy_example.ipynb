{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb6d6bb5-841a-44c9-bc0d-dfb8c7d5bd0b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d1c45e-79b1-49da-832e-e5b14ceb92ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# === Install & imports ===\n",
    "# (Re-run if your environment is fresh)\n",
    "\n",
    "# %%capture\n",
    "%pip install -q dspy-ai pandas numpy"
   ]
  },
  {
   "cell_type": "raw",
   "id": "acc8a676-e2d3-4b20-b7f6-f1bb37e7a2b8",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sample data for the training_examples.csv file\n",
    "data = [\n",
    "    {\n",
    "        \"initial_prompt\": \"Summarize the input text into one concise sentence.\",\n",
    "        \"input_text\": \"The movie was incredibly well-acted and had stunning cinematography, but the plot was predictable.\",\n",
    "        \"ideal_output\": \"Great acting and visuals, but a predictable plot.\"\n",
    "    },\n",
    "    {\n",
    "        \"initial_prompt\": \"Summarize the input text into one concise sentence.\",\n",
    "        \"input_text\": \"The new restaurant offers delicious food but suffers from slow service during peak hours.\",\n",
    "        \"ideal_output\": \"Delicious food with slow peak-time service.\"\n",
    "    },\n",
    "    {\n",
    "        \"initial_prompt\": \"Summarize the input text into one concise sentence.\",\n",
    "        \"input_text\": \"The book was filled with thrilling twists, although the ending felt rushed.\",\n",
    "        \"ideal_output\": \"Thrilling story but a rushed ending.\"\n",
    "    },\n",
    "    {\n",
    "        \"initial_prompt\": \"Summarize the input text into one concise sentence.\",\n",
    "        \"input_text\": \"The concert had an amazing atmosphere and great performances, but the sound quality was inconsistent.\",\n",
    "        \"ideal_output\": \"Amazing concert vibe with uneven sound quality.\"\n",
    "    },\n",
    "    {\n",
    "        \"initial_prompt\": \"Summarize the input text into one concise sentence.\",\n",
    "        \"input_text\": \"The app is user-friendly and fast, yet it lacks some essential customization options.\",\n",
    "        \"ideal_output\": \"Fast and user-friendly app missing key customization.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Save to CSV\n",
    "file_path = r\"C:\\Users\\nalla\\OneDrive\\AI_Research\\2025\\GenAI\\DSPy\\training_examples.csv\"\n",
    "df.to_csv(file_path, index=False)\n",
    "\n",
    "file_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a7e133-6db7-4494-9f79-82a0545cbe2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb81ada-2d75-4922-acbf-9929f98aa1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import os\n",
    "import random\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import dspy\n",
    "\n",
    "# ==============================\n",
    "# CONFIG\n",
    "# ==============================\n",
    "# Path to your input CSV:\n",
    "INPUT_CSV  = \"training_examples.csv\"\n",
    "# Where to save the results:\n",
    "OUTPUT_CSV = \"optimized_prompt_with_inputs.csv\"\n",
    "# Optional: set your LLM (change to what you use; requires proper credentials)\n",
    "# Examples:\n",
    "#   dspy.LM(\"openai/gpt-4o-mini\")  -> needs OPENAI_API_KEY\n",
    "#   dspy.LM(\"anthropic/claude-3-5-sonnet\") -> needs ANTHROPIC_API_KEY\n",
    "#   dspy.LM(\"ollama/llama3\") -> if you run Ollama locally\n",
    "MODEL_NAME = \"openai/gpt-4o-mini\"\n",
    "\n",
    "# Fixed random seed for reproducibility of example selection\n",
    "SEED = 13\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# ==============================\n",
    "# READ DATA\n",
    "# ==============================\n",
    "df = pd.read_csv(INPUT_CSV).fillna(\"\")\n",
    "\n",
    "required_cols = {\"initial_prompt\", \"input_text\", \"ideal_output\"}\n",
    "missing = required_cols - set(df.columns.str.lower())\n",
    "if missing:\n",
    "    raise ValueError(\n",
    "        f\"Your CSV must contain columns: {sorted(required_cols)}; missing: {sorted(missing)}\"\n",
    "    )\n",
    "\n",
    "# Normalize column names just in case\n",
    "colmap = {c: c.lower() for c in df.columns}\n",
    "df.rename(columns=colmap, inplace=True)\n",
    "\n",
    "if len(df) < 4:\n",
    "    print(\n",
    "        \"⚠️ Tip: Add more rows for better optimization. \"\n",
    "        \"DSPy works best with a handful to dozens of labeled examples.\"\n",
    "    )\n",
    "\n",
    "# Use the first row's initial_prompt as the instruction seed\n",
    "instruction_text = df.iloc[0][\"initial_prompt\"].strip()\n",
    "if not instruction_text:\n",
    "    instruction_text = \"Perform the task described using the input and produce the ideal output.\"\n",
    "\n",
    "# ==============================\n",
    "# CONFIGURE DSPy\n",
    "# ==============================\n",
    "# Make sure your API key is available if you use a hosted model (e.g., OpenAI/Anthropic).\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"sk-...\"  # uncomment and set if needed\n",
    "\n",
    "dspy.settings.configure(lm=dspy.LM(MODEL_NAME, max_tokens=512))\n",
    "\n",
    "# ------------------------------\n",
    "# Define a Signature and Module\n",
    "# ------------------------------\n",
    "class TaskSignature(dspy.Signature):\n",
    "    \"\"\"{instruction}\"\"\"\n",
    "    input_text: str = dspy.InputField(desc=\"The input for the task.\")\n",
    "    prediction: str = dspy.OutputField(desc=\"The ideal output for the task.\")\n",
    "\n",
    "# Inject the instruction into the signature docstring dynamically:\n",
    "TaskSignature.__doc__ = instruction_text or TaskSignature.__doc__\n",
    "\n",
    "# A simple module that maps input_text -> prediction\n",
    "task_module = dspy.Predict(TaskSignature)\n",
    "\n",
    "# ==============================\n",
    "# BUILD TRAIN/DEV SETS\n",
    "# ==============================\n",
    "# Convert rows to DSPy Examples\n",
    "examples = [\n",
    "    dspy.Example(input_text=row[\"input_text\"], prediction=row[\"ideal_output\"]).with_inputs(\"input_text\")\n",
    "    for _, row in df.iterrows()\n",
    "]\n",
    "\n",
    "# Small, simple split (80/20); for tiny datasets, DSPy can still run but with variance.\n",
    "if len(examples) >= 5:\n",
    "    split = int(0.8 * len(examples))\n",
    "else:\n",
    "    split = max(1, len(examples) - 1)\n",
    "\n",
    "trainset = examples[:split]\n",
    "devset   = examples[split:] if (len(examples) - split) > 0 else examples[:1]\n",
    "\n",
    "# ==============================\n",
    "# DEFINE A METRIC\n",
    "# ==============================\n",
    "def token_f1(prediction: str, gold: str) -> float:\n",
    "    \"\"\"Simple, model-agnostic F1 over whitespace tokens.\"\"\"\n",
    "    p = prediction.lower().split()\n",
    "    g = gold.lower().split()\n",
    "    if not p and not g:\n",
    "        return 1.0\n",
    "    if not p or not g:\n",
    "        return 0.0\n",
    "    p_set, g_set = set(p), set(g)\n",
    "    inter = len(p_set & g_set)\n",
    "    if inter == 0:\n",
    "        return 0.0\n",
    "    precision = inter / (len(p_set) + 1e-9)\n",
    "    recall    = inter / (len(g_set) + 1e-9)\n",
    "    return 2 * precision * recall / (precision + recall + 1e-9)\n",
    "\n",
    "def metric_fn(example, pred) -> float:\n",
    "    \"\"\"DSPy metric: higher is better. Compare predicted text to ideal_output.\"\"\"\n",
    "    return token_f1(pred.prediction, example.prediction)\n",
    "\n",
    "# ==============================\n",
    "# OPTIMIZE THE PROMPT WITH DSPy\n",
    "# ==============================\n",
    "# BootstrapFewShot selects and arranges few‑shot examples to improve the prompt\n",
    "# You can tune these knobs for your dataset size/time budget:\n",
    "optimizer = dspy.BootstrapFewShot(\n",
    "    metric=metric_fn,\n",
    "    max_bootstrapped_demos=4,  # synthetic demos\n",
    "    max_labeled_demos=16,      # uses your CSV examples\n",
    "    max_rounds=3\n",
    ")\n",
    "\n",
    "compiled = optimizer.compile(\n",
    "    student=task_module,\n",
    "    trainset=trainset,\n",
    "    valset=devset\n",
    ")\n",
    "\n",
    "# ==============================\n",
    "# EXTRACT THE \"IMPROVED PROMPT\"\n",
    "# ==============================\n",
    "# DSPy does not expose a single raw \"prompt string\" API because it abstracts prompts,\n",
    "# but we can reconstruct a faithful, deployable template from:\n",
    "#   - the (possibly refined) instruction\n",
    "#   - the selected few‑shot demos (compiled.demos)\n",
    "#   - a canonical input/output format\n",
    "#\n",
    "# This string is what you can reuse elsewhere if you want a plain prompt.\n",
    "\n",
    "def make_prompt_string(program: dspy.Module) -> str:\n",
    "    # Instruction (signature docstring can be refined by some optimizers)\n",
    "    instr = getattr(program, \"signature\", None)\n",
    "    instr_text = \"\"\n",
    "    if instr is not None:\n",
    "        instr_text = getattr(instr, \"doc\", \"\") or getattr(instr, \"__doc__\", \"\") or \"\"\n",
    "\n",
    "    # Demos (few-shot examples chosen/bootstrapped by the optimizer)\n",
    "    demos = []\n",
    "    # compiled.demos may be dspy.Example objects or dicts (after save/load).\n",
    "    for demo in getattr(program, \"demos\", []):\n",
    "        if hasattr(demo, \"toDict\"):\n",
    "            d = demo.toDict()\n",
    "        elif isinstance(demo, dict):\n",
    "            d = demo\n",
    "        else:\n",
    "            # best-effort fallback\n",
    "            d = {\"input_text\": getattr(demo, \"input_text\", \"\"), \"prediction\": getattr(demo, \"prediction\", \"\")}\n",
    "        demos.append(d)\n",
    "\n",
    "    lines = []\n",
    "    lines.append(\"### Instruction\")\n",
    "    lines.append(instr_text.strip() or \"Use the input_text to produce the best possible prediction.\\n\")\n",
    "    if demos:\n",
    "        lines.append(\"\\n### Few‑Shot Examples\")\n",
    "        for i, d in enumerate(demos, 1):\n",
    "            lines.append(f\"\\n# Example {i}\")\n",
    "            lines.append(f\"Input:\\n{d.get('input_text','').strip()}\")\n",
    "            lines.append(f\"Output:\\n{d.get('prediction','').strip()}\")\n",
    "\n",
    "    # Canonical I/O template\n",
    "    lines.append(\"\\n### Now answer for this new input\")\n",
    "    lines.append(\"Input:\\n{input_text}\")\n",
    "    lines.append(\"Output:\")\n",
    "    return \"\\n\".join(lines).strip()\n",
    "\n",
    "optimized_prompt_str = make_prompt_string(compiled)\n",
    "\n",
    "# ==============================\n",
    "# SAVE: final improved prompt + inputs\n",
    "# ==============================\n",
    "out = pd.DataFrame({\n",
    "    \"optimized_prompt\": [optimized_prompt_str] * len(df),\n",
    "    \"input_text\": df[\"input_text\"].tolist(),\n",
    "    \"ideal_output\": df[\"ideal_output\"].tolist()\n",
    "})\n",
    "\n",
    "out.to_csv(OUTPUT_CSV, index=False)\n",
    "\n",
    "print(\"✅ Done!\")\n",
    "print(f\"- Seed instruction (from CSV):\\n{instruction_text}\\n\")\n",
    "print(f\"- Optimized prompt written to: {OUTPUT_CSV}\")\n",
    "print(\"\\n--- Preview of the optimized prompt ---\\n\")\n",
    "print(optimized_prompt_str[:2000] + (\"\\n...[truncated]...\" if len(optimized_prompt_str) > 2000 else \"\"))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
