{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c84a5e4-4a9f-4136-b71c-499f5f5586af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "#!{sys.executable} -m pip install --upgrade deepeval openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5372043e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.10.0\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b98283a8-3526-4bd5-82af-b3b0fc7cbd01",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a5126f5a-1ce6-493c-ba08-422a9c42942c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your GOOGLEAI_API_KEY:  ········\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import getpass\n",
    "os.environ[\"GOOGLEAI_API_KEY\"] = getpass.getpass(\n",
    "    prompt=\"Enter your GOOGLEAI_API_KEY: \"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a6fee3a5-98b2-4330-9ba0-f8fee17fb24f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from deepeval.models import GeminiModel\n",
    "from deepeval.test_case import LLMTestCase\n",
    "from deepeval.metrics import FaithfulnessMetric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a9986036-298a-4b71-9243-2b080941b612",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create a GeminiModel with your AI Studio key  \n",
    "EVAL_MODEL        = \"gemini-1.5-pro\"\n",
    "GOOGLEAI_API_KEY  = os.environ[\"GOOGLEAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6266739e-bc05-44b9-b346-04422a4a5581",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_model = GeminiModel(\n",
    "    model_name=EVAL_MODEL,\n",
    "    api_key=GOOGLEAI_API_KEY\n",
    ")\n",
    "\n",
    "## Use that model in your FaithfulnessMetric  \n",
    "metric = FaithfulnessMetric(\n",
    "    model=eval_model,\n",
    "    include_reason=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a923a925-6560-4dab-aa94-0bc070283d06",
   "metadata": {},
   "source": [
    "## 1. Misinformation baked into the context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8956424b-0773-4a4d-ad6b-81e344582e26",
   "metadata": {},
   "source": [
    "The context itself is simply wrong, so a perfectly “grounded” answer is wrong too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f58335ad-dc27-45e1-b81b-0c77576bf52c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0fb93f2194e4aca82166e47737ae486",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0 The score is 1.00 because the actual output aligns perfectly with the retrieval context, with no contradictions present.\n"
     ]
    }
   ],
   "source": [
    "tc = LLMTestCase(\n",
    "    input=\"What is the boiling point of water at sea level in Celsius?\",\n",
    "    retrieval_context=[\n",
    "        ## Flat-out misinformation\n",
    "        \"At standard atmospheric pressure, water boils at 110 °C.\"\n",
    "    ],\n",
    "    actual_output=\"Water boils at 110 °C at sea level.\"\n",
    ")\n",
    "\n",
    "metric.measure(tc)\n",
    "print(metric.score, metric.reason)   # ≈ 1.00, “All claims supported…”"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd633dab-b450-4cd5-9e8a-438c5a5b6817",
   "metadata": {},
   "source": [
    "## 2. Invented co-author (hallucination lives only in the context)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db674830-42a3-4e23-a559-2009f97f8d46",
   "metadata": {},
   "source": [
    "Context adds a spurious co-author; the answer repeats it → still 1.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "47c115fa-b19b-44e0-99ef-dd732399a81c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84c38e16f2c34820a8db35550d0b4a61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0 The score is 1.00 because the actual output aligns perfectly with the retrieval context, with no contradictions present.\n"
     ]
    }
   ],
   "source": [
    "tc = LLMTestCase(\n",
    "    input=\"Who wrote the novel *Frankenstein*?\",\n",
    "    retrieval_context=[\n",
    "        # Hallucinated collaborator\n",
    "        \"Mary Shelley co-wrote *Frankenstein* with Lord Byron in 1818.\"\n",
    "    ],\n",
    "    actual_output=\"*Frankenstein* was co-written by Mary Shelley and Lord Byron.\"\n",
    ")\n",
    "\n",
    "metric.measure(tc)\n",
    "print(metric.score, metric.reason)   # ≈ 1.00"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd4bc8b-d9a0-4efc-8415-8077890672d2",
   "metadata": {},
   "source": [
    "## 3. Out-of-date context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "003108ca-2ed3-434d-bf05-2b1fc7cef5a8",
   "metadata": {},
   "source": [
    "Context was true once, but the world has changed. Answer is outdated yet perfectly faithful."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a54a5f-33d4-49d1-9d1d-9fade4d705d0",
   "metadata": {},
   "source": [
    "Context was true once, but the world has changed. Answer is outdated yet perfectly faithful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d0f7338a-02ce-430a-8c17-4cf03a87ee8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e58f5e4f705a43fb969713b3341cf83f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0 The score is 1.00 because the actual output aligns perfectly with the retrieval context, with no contradictions present.\n"
     ]
    }
   ],
   "source": [
    "tc = LLMTestCase(\n",
    "    input=\"Who is the current CEO of Amazon?\",\n",
    "    retrieval_context=[\n",
    "        # Fact that used to be true\n",
    "        \"Jeff Bezos is the current CEO of Amazon.\"\n",
    "    ],\n",
    "    actual_output=\"Jeff Bezos is the current CEO of Amazon.\"\n",
    ")\n",
    "\n",
    "metric.measure(tc)\n",
    "print(metric.score, metric.reason)   # ≈ 1.00"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5fb7e10-3e8f-4014-9578-51ec6a582c1c",
   "metadata": {},
   "source": [
    "### Take-away\n",
    "\n",
    "FaithfulnessMetric cares only about internal consistency between answer ⇄ context.\n",
    "To guard against externally incorrect contexts you also need a complementary truthfulness or reference-based QA metric—or you must ensure your retrieval pipeline returns authoritative context in the first place.\n",
    "\n",
    "Use a second metric (e.g. truthfulness, reference-based QA, or retrieval precision) alongside faithfulness, or make sure your retrieval step always pulls authoritative documents. That combination catches both hallucinations and poisoned context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3709fd9a-9c90-4adf-b732-58bef5368c67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a4c431c6deb4d8c994e196c2c61aeb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a7ecf93a73e44f48c4166e9a1606ff7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unfaithful   ⇢ score=0.00 • reason=The score is 0.00 because the actual output incorrectly attributes the authorship of \"Nineteen Eighty-Four\" to Ernest Hemingway instead of the true author, George Orwell.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d87041222eb4807ad8b8a5e5e51c600",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "partly true  ⇢ score=0.00 • reason=The score is 0.00 because the actual output incorrectly states that Aldous Huxley co-authored \"1984\" with George Orwell.  The provided context clearly indicates that George Orwell was the sole author.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "faithful     ⇢ score=1.00 • reason=The score is 1.00 because the actual output aligns perfectly with the retrieval context, with no contradictions present.\n"
     ]
    }
   ],
   "source": [
    "## Keep the query + context constant  \n",
    "query = \"Who wrote the novel '1984'?\"\n",
    "retrieval_context = [\n",
    "    \"George Orwell wrote the dystopian novel 'Nineteen Eighty-Four' (often \"\n",
    "    \"just called '1984'); it was published in 1949.\"\n",
    "]\n",
    "\n",
    "## Create three answers of increasing factual quality \n",
    "answers = {\n",
    "    \"unfaithful\":  \"Ernest Hemingway wrote 1984.\",                           # ≈ 0.00\n",
    "    \"partly true\": \"George Orwell wrote 1984 together with Aldous Huxley.\",  # ≈ 0.50\n",
    "    \"faithful\":    \"George Orwell wrote 1984.\"                               # ≈ 1.00\n",
    "}\n",
    "\n",
    "## Evaluate each answer \n",
    "for label, answer in answers.items():\n",
    "    tc = LLMTestCase(\n",
    "        input=query,\n",
    "        actual_output=answer,\n",
    "        retrieval_context=retrieval_context\n",
    "    )\n",
    "    metric.measure(tc)\n",
    "    print(f\"{label:12} ⇢ score={metric.score:.2f} • reason={metric.reason}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2e6886a0-a4ab-4639-9e59-5c06bc7c2da9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f37a62381f84931bf04636a70289921",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "faithfulness=0.50 • reason=The score is 0.50 because the actual output incorrectly states the length of the Great Wall of China as 10,000 kilometers, contradicting the provided text which clearly indicates its length as approximately 21,196 kilometers. This significant discrepancy warrants a lower faithfulness score.\n"
     ]
    }
   ],
   "source": [
    "## Query and retrieval context with two atomic facts\n",
    "query = \"Give me two facts about the Great Wall of China.\"\n",
    "retrieval_context = [\n",
    "    # Fact A – dynasty\n",
    "    \"The Great Wall of China was completed mainly during the Ming Dynasty.\",\n",
    "    # Fact B – length\n",
    "    \"The Great Wall of China is about 21,196 kilometres long.\"\n",
    "]\n",
    "\n",
    "## Answer: keeps Fact A correct but muddles Fact B\n",
    "answer = (\n",
    "    \"The Great Wall of China was completed mainly during the Ming Dynasty \"\n",
    "    \"and is about 10,000 kilometres long.\"\n",
    ")\n",
    "\n",
    "##  Evaluate\n",
    "tc = LLMTestCase(\n",
    "    input=query,\n",
    "    actual_output=answer,\n",
    "    retrieval_context=retrieval_context\n",
    ")\n",
    "metric.measure(tc)\n",
    "print(f\"faithfulness={metric.score:.2f} • reason={metric.reason}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca34de1-86a0-4bbc-9f1f-129af56038ac",
   "metadata": {},
   "source": [
    "#### Supported claims ÷ total claims = 1 ÷ 2 = 0.5.\n",
    "Because DeepEval breaks the answer into these two atomic assertions, the judge LLM usually assigns a faithfulness score very close to 0.5 and explains that one claim aligns while the other conflicts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c875b4-ed1b-4dc5-b7bd-d4331cc74f7a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10",
   "language": "python",
   "name": "py310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
