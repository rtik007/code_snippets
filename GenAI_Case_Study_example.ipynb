{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ec783b2-2fc3-4946-9195-b03499d6a91e",
   "metadata": {},
   "source": [
    "Give me 5 case studies on gen ai eval coding interview questions with answers jupyter notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feebf6a9-ab3f-4a7e-a8da-9d3a83a492c0",
   "metadata": {},
   "source": [
    "Case Study 1: Text Generation Evaluation\n",
    "Problem:\n",
    "Evaluate the quality of generated text from a Large Language Model (LLM) for a specific task (e.g., summarization, creative writing, chatbot responses).\n",
    "Coding Task:\n",
    "Implement metrics like BLEU, ROUGE, or perplexity to quantitatively assess text quality. Demonstrate how to compare different LLM outputs based on these metrics.\n",
    "Jupyter Notebook Focus:\n",
    "Data loading, text preprocessing, function implementations for metrics, visualization of scores, and qualitative analysis of generated text examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20c9fccc-0246-445d-a5d3-9672604bde39",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'nltk'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mseaborn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msns\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtranslate\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbleu_score\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m sentence_bleu, SmoothingFunction\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mrouge_score\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m rouge_scorer\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GPT2Tokenizer, GPT2LMHeadModel\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'nltk'"
     ]
    }
   ],
   "source": [
    "### Text Generation Evaluation Notebook\n",
    "\n",
    "# Import Required Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from rouge_score import rouge_scorer\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "import torch\n",
    "\n",
    "# Set style for plots\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "### 1. Load and Explore Data\n",
    "# Simulated generated outputs from two different LLMs for summarization task\n",
    "\n",
    "data = pd.DataFrame({\n",
    "    'reference': [\n",
    "        \"The cat sat on the mat and looked out the window.\",\n",
    "        \"The stock market crashed due to unexpected economic news.\",\n",
    "        \"The book explores themes of love and loss in a post-war setting.\"\n",
    "    ],\n",
    "    'LLM_A': [\n",
    "        \"The cat sat on a mat and stared out the window.\",\n",
    "        \"Economic news led to a crash in the stock market.\",\n",
    "        \"This novel is about love and loss after the war.\"\n",
    "    ],\n",
    "    'LLM_B': [\n",
    "        \"A cat was looking out the window while sitting on a mat.\",\n",
    "        \"The financial markets tumbled after the news.\",\n",
    "        \"The book discusses war, romance, and grief.\"\n",
    "    ]\n",
    "})\n",
    "\n",
    "data.head()\n",
    "\n",
    "### 2. Preprocessing (if necessary)\n",
    "# Here we assume text is already clean for simplicity\n",
    "\n",
    "### 3. Evaluation Metric Functions\n",
    "\n",
    "def compute_bleu(reference, candidate):\n",
    "    ref_tokens = reference.split()\n",
    "    cand_tokens = candidate.split()\n",
    "    smoothie = SmoothingFunction().method4\n",
    "    return sentence_bleu([ref_tokens], cand_tokens, smoothing_function=smoothie)\n",
    "\n",
    "def compute_rouge(reference, candidate):\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)\n",
    "    scores = scorer.score(reference, candidate)\n",
    "    return scores\n",
    "\n",
    "def compute_perplexity(texts, model, tokenizer):\n",
    "    encodings = tokenizer(texts, return_tensors='pt', padding=True, truncation=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**encodings, labels=encodings['input_ids'])\n",
    "    return torch.exp(outputs.loss).item()\n",
    "\n",
    "### 4. Compute Scores\n",
    "\n",
    "bleu_scores_A = [compute_bleu(ref, pred) for ref, pred in zip(data['reference'], data['LLM_A'])]\n",
    "bleu_scores_B = [compute_bleu(ref, pred) for ref, pred in zip(data['reference'], data['LLM_B'])]\n",
    "\n",
    "rouge_1_A = [compute_rouge(ref, pred)['rouge1'].fmeasure for ref, pred in zip(data['reference'], data['LLM_A'])]\n",
    "rouge_1_B = [compute_rouge(ref, pred)['rouge1'].fmeasure for ref, pred in zip(data['reference'], data['LLM_B'])]\n",
    "\n",
    "rouge_L_A = [compute_rouge(ref, pred)['rougeL'].fmeasure for ref, pred in zip(data['reference'], data['LLM_A'])]\n",
    "rouge_L_B = [compute_rouge(ref, pred)['rougeL'].fmeasure for ref, pred in zip(data['reference'], data['LLM_B'])]\n",
    "\n",
    "# Load GPT2 for perplexity scoring (simplified, not optimal for long texts)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "model.eval()\n",
    "\n",
    "perplexity_A = compute_perplexity(list(data['LLM_A']), model, tokenizer)\n",
    "perplexity_B = compute_perplexity(list(data['LLM_B']), model, tokenizer)\n",
    "\n",
    "### 5. Visualize Scores\n",
    "\n",
    "score_df = pd.DataFrame({\n",
    "    'BLEU_LLM_A': bleu_scores_A,\n",
    "    'BLEU_LLM_B': bleu_scores_B,\n",
    "    'ROUGE1_LLM_A': rouge_1_A,\n",
    "    'ROUGE1_LLM_B': rouge_1_B,\n",
    "    'ROUGE_L_LLM_A': rouge_L_A,\n",
    "    'ROUGE_L_LLM_B': rouge_L_B\n",
    "})\n",
    "\n",
    "score_df.plot(kind='bar', figsize=(12,6))\n",
    "plt.title(\"Comparison of LLM A and LLM B Across BLEU and ROUGE Metrics\")\n",
    "plt.xlabel(\"Example Index\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.xticks(rotation=0)\n",
    "plt.legend(loc='lower right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "### 6. Qualitative Examples\n",
    "for i in range(len(data)):\n",
    "    print(f\"Example {i+1}\")\n",
    "    print(\"Reference: \", data['reference'][i])\n",
    "    print(\"LLM_A: \", data['LLM_A'][i])\n",
    "    print(\"LLM_B: \", data['LLM_B'][i])\n",
    "    print(f\"BLEU_A: {bleu_scores_A[i]:.2f}, BLEU_B: {bleu_scores_B[i]:.2f}\")\n",
    "    print(f\"ROUGE-L_A: {rouge_L_A[i]:.2f}, ROUGE-L_B: {rouge_L_B[i]:.2f}\\n\")\n",
    "\n",
    "### 7. Perplexity Summary\n",
    "print(f\"\\nPerplexity (LLM_A): {perplexity_A:.2f}\")\n",
    "print(f\"Perplexity (LLM_B): {perplexity_B:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d68584-3b7d-465f-9bdf-dc44667fd193",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (portfolio_selection)",
   "language": "python",
   "name": "portfolio_selection"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
